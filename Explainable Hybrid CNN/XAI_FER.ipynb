{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics==8.* opencv-python matplotlib grad-cam\n",
        "\n",
        "import os, glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "Mwj4fwm5be8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9537f013-a411-451a-9fa9-7fb07ae7a6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/7.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m146.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Torch: 2.9.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/images\"   # <- change this (folder has best.pt and images)\n",
        "WEIGHTS  = os.path.join(DATA_DIR, \"hybrid_wbf.pt\")\n",
        "\n",
        "OUT_DIR  = \"/content/outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Collect images\n",
        "img_exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.webp\")\n",
        "image_paths = []\n",
        "for e in img_exts:\n",
        "    image_paths += glob.glob(os.path.join(DATA_DIR, e))\n",
        "image_paths = sorted(image_paths)\n",
        "\n",
        "print(\"Weights:\", WEIGHTS, \"exists:\", os.path.exists(WEIGHTS))\n",
        "print(\"Found images:\", len(image_paths))\n",
        "for p in image_paths:\n",
        "    print(\" -\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGIh2D8ybhMg",
        "outputId": "6c2d9159-8a67-4442-d664-36e932e0bbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: /content/images/hybrid_wbf.pt exists: True\n",
            "Found images: 2\n",
            " - /content/images/emotion_female.png\n",
            " - /content/images/emotion_male.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ensemble-boxes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLUicz7VPV7i",
        "outputId": "9bcfdf17-4476-4b3c-db6a-e561ce4b9800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ensemble-boxes\n",
            "  Downloading ensemble_boxes-1.0.9-py3-none-any.whl.metadata (728 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ensemble-boxes) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ensemble-boxes) (2.2.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from ensemble-boxes) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->ensemble-boxes) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ensemble-boxes) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ensemble-boxes) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ensemble-boxes) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ensemble-boxes) (1.17.0)\n",
            "Downloading ensemble_boxes-1.0.9-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: ensemble-boxes\n",
            "Successfully installed ensemble-boxes-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "\n",
        "# ----------------------------\n",
        "# Paths\n",
        "# ----------------------------\n",
        "WEIGHTS = \"/content/hybrid_out/hybrid_wbf.pt\"\n",
        "OUT_DIR = \"/content/hybrid_out\"\n",
        "pred_dir = os.path.join(OUT_DIR, \"pred\")\n",
        "os.makedirs(pred_dir, exist_ok=True)\n",
        "\n",
        "# image_paths = [...]  # list of image paths\n",
        "\n",
        "# ----------------------------\n",
        "# Load hybrid checkpoint (NOT via YOLO)\n",
        "# ----------------------------\n",
        "ckpt = torch.load(WEIGHTS, map_location=\"cpu\")\n",
        "\n",
        "# ✅ CORRECT KEYS\n",
        "w8  = ckpt[\"yolov8_weight_path\"]\n",
        "w11 = ckpt[\"yolov11_weight_path\"]\n",
        "\n",
        "fusion_cfg = ckpt[\"fusion\"]\n",
        "wbf_weights = tuple(fusion_cfg.get(\"weights\", (1.0, 1.0)))\n",
        "wbf_iou     = float(fusion_cfg.get(\"iou_thr\", 0.55))\n",
        "wbf_skip    = float(fusion_cfg.get(\"skip_box_thr\", 0.001))\n",
        "\n",
        "# ----------------------------\n",
        "# Load YOLO models\n",
        "# ----------------------------\n",
        "device = 0  # GPU\n",
        "m8  = YOLO(w8)\n",
        "m11 = YOLO(w11)\n",
        "\n",
        "names = m8.names  # original class names\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def yolo_to_norm(res, w, h):\n",
        "    if res.boxes is None or len(res.boxes) == 0:\n",
        "        return [], [], []\n",
        "    boxes = res.boxes.xyxy.detach().cpu().numpy()\n",
        "    scores = res.boxes.conf.detach().cpu().numpy()\n",
        "    labels = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "\n",
        "    boxes[:, [0, 2]] /= w\n",
        "    boxes[:, [1, 3]] /= h\n",
        "    boxes = np.clip(boxes, 0.0, 1.0)\n",
        "    return boxes.tolist(), scores.tolist(), labels.tolist()\n",
        "\n",
        "def fuse_wbf(res8, res11, w, h):\n",
        "    b8, s8, l8 = yolo_to_norm(res8, w, h)\n",
        "    b11, s11, l11 = yolo_to_norm(res11, w, h)\n",
        "\n",
        "    boxes, scores, labels = weighted_boxes_fusion(\n",
        "        [b8, b11],\n",
        "        [s8, s11],\n",
        "        [l8, l11],\n",
        "        weights=list(wbf_weights),\n",
        "        iou_thr=wbf_iou,\n",
        "        skip_box_thr=wbf_skip\n",
        "    )\n",
        "\n",
        "    boxes = np.array(boxes)\n",
        "    scores = np.array(scores)\n",
        "    labels = np.array(labels, dtype=int)\n",
        "\n",
        "    if len(boxes) > 0:\n",
        "        boxes[:, [0, 2]] *= w\n",
        "        boxes[:, [1, 3]] *= h\n",
        "\n",
        "    return boxes, scores, labels\n",
        "\n",
        "def draw_boxes(img, boxes, scores, labels, conf_thr=0.25):\n",
        "    out = img.copy()\n",
        "    for (x1, y1, x2, y2), sc, lb in zip(boxes, scores, labels):\n",
        "        if sc < conf_thr:\n",
        "            continue\n",
        "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "        name = names[lb] if isinstance(names, dict) else names[lb]\n",
        "        cv2.rectangle(out, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(out, f\"{name} {sc:.2f}\", (x1, max(15, y1 - 5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "    return out\n",
        "\n",
        "# ----------------------------\n",
        "# Run Hybrid Prediction\n",
        "# ----------------------------\n",
        "for p in image_paths:\n",
        "    img = cv2.imread(p)\n",
        "    if img is None:\n",
        "        continue\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    r8  = m8.predict(p, imgsz=640, conf=0.001, device=device, verbose=False)[0]\n",
        "    r11 = m11.predict(p, imgsz=640, conf=0.001, device=device, verbose=False)[0]\n",
        "\n",
        "    boxes, scores, labels = fuse_wbf(r8, r11, w, h)\n",
        "\n",
        "    vis = draw_boxes(img, boxes, scores, labels)\n",
        "\n",
        "    out_path = os.path.join(pred_dir, os.path.splitext(os.path.basename(p))[0] + \".jpg\")\n",
        "    cv2.imwrite(out_path, vis)\n",
        "\n",
        "print(\"✅ Saved HYBRID predictions into:\", pred_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6Gff2XzbtKm",
        "outputId": "9b156459-d325-4767-b2a3-7ec2a4cb3467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved HYBRID predictions into: /content/hybrid_out/pred\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from pytorch_grad_cam import GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# ============================================================\n",
        "# Hybrid Grad-CAM++ (YOLOv8 + YOLOv11) → fused heatmap\n",
        "# Requires: m8, m11 already loaded as ultralytics.YOLO objects\n",
        "# Requires: image_paths (list of image paths), OUT_DIR (output root)\n",
        "# ============================================================\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---- helper: find a good conv layer automatically (last Conv2d with spatial map > 1x1) ----\n",
        "def find_last_good_conv2d(torch_model, input_shape=(1,3,640,640), device=\"cuda\"):\n",
        "    acts, hooks = [], []\n",
        "\n",
        "    def hook_fn(m, inp, out):\n",
        "        if isinstance(out, torch.Tensor) and out.ndim == 4:\n",
        "            acts.append((m, out.shape))  # (module, [B,C,H,W])\n",
        "\n",
        "    for m in torch_model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            hooks.append(m.register_forward_hook(hook_fn))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        dummy = torch.zeros(*input_shape, device=device)\n",
        "        _ = torch_model(dummy)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    good = [x for x in acts if x[1][2] > 1 and x[1][3] > 1]  # H,W > 1\n",
        "    return good[-1][0] if good else (acts[-1][0] if acts else None)\n",
        "\n",
        "# ---- helper: letterbox resize like YOLO ----\n",
        "def letterbox(im, new_shape=640, color=(114,114,114)):\n",
        "    h, w = im.shape[:2]\n",
        "    r = min(new_shape / h, new_shape / w)\n",
        "    nh, nw = int(round(h * r)), int(round(w * r))\n",
        "    im_resized = cv2.resize(im, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "    canvas = np.full((new_shape, new_shape, 3), color, dtype=np.uint8)\n",
        "    top = (new_shape - nh) // 2\n",
        "    left = (new_shape - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = im_resized\n",
        "    return canvas\n",
        "\n",
        "def to_tensor_yolo(img_bgr_640, device=\"cuda\"):\n",
        "    img_rgb = cv2.cvtColor(img_bgr_640, cv2.COLOR_BGR2RGB)\n",
        "    img_float = img_rgb.astype(np.float32) / 255.0\n",
        "    x = torch.from_numpy(img_float).permute(2,0,1).unsqueeze(0).to(device)  # [1,3,640,640]\n",
        "    return x, img_float  # tensor, rgb_float\n",
        "\n",
        "# ---- IMPORTANT: use the raw torch models for gradient flow ----\n",
        "torch_m8  = m8.model.to(device).eval()\n",
        "torch_m11 = m11.model.to(device).eval()\n",
        "\n",
        "# ---- pick CAM layers for both models ----\n",
        "layer8  = find_last_good_conv2d(torch_m8,  input_shape=(1,3,640,640), device=device)\n",
        "layer11 = find_last_good_conv2d(torch_m11, input_shape=(1,3,640,640), device=device)\n",
        "if layer8 is None or layer11 is None:\n",
        "    raise RuntimeError(\"Could not find suitable Conv2d layer(s) for CAM.\")\n",
        "\n",
        "print(\"YOLOv8 CAM layer :\", layer8)\n",
        "print(\"YOLOv11 CAM layer:\", layer11)\n",
        "\n",
        "cam8  = GradCAMPlusPlus(model=torch_m8,  target_layers=[layer8])\n",
        "cam11 = GradCAMPlusPlus(model=torch_m11, target_layers=[layer11])\n",
        "\n",
        "cam_dir = os.path.join(OUT_DIR, \"hybrid_gradcampp\")\n",
        "os.makedirs(cam_dir, exist_ok=True)\n",
        "\n",
        "# ---- CAM target that DEPENDS on model output (keeps grad graph) ----\n",
        "class YoloMaxScoreTarget:\n",
        "    def __call__(self, model_output):\n",
        "        # unwrap nested outputs\n",
        "        if isinstance(model_output, (list, tuple)):\n",
        "            model_output = model_output[0]\n",
        "            if isinstance(model_output, (list, tuple)):\n",
        "                model_output = model_output[0]\n",
        "\n",
        "        if not torch.is_tensor(model_output):\n",
        "            raise RuntimeError(\"Model output is not a torch.Tensor\")\n",
        "\n",
        "        if model_output.ndim == 2:\n",
        "            model_output = model_output.unsqueeze(0)  # [1, N, D]\n",
        "\n",
        "        d = model_output.shape[-1]\n",
        "        if d <= 6:\n",
        "            return model_output.max()\n",
        "\n",
        "        score_a = model_output[..., 4:].max()\n",
        "        obj = model_output[..., 4]\n",
        "        if d > 5:\n",
        "            cls = model_output[..., 5:]\n",
        "            score_b = (obj.unsqueeze(-1) * cls).max()\n",
        "        else:\n",
        "            score_b = obj.max()\n",
        "\n",
        "        return torch.maximum(score_a, score_b)\n",
        "\n",
        "# ---- hybrid CAM fusion weights (use your WBF weights if available) ----\n",
        "try:\n",
        "    w8_cam, w11_cam = float(wbf_weights[0]), float(wbf_weights[1])  # if you defined wbf_weights earlier\n",
        "except Exception:\n",
        "    w8_cam, w11_cam = 1.0, 1.0\n",
        "w_sum = w8_cam + w11_cam + 1e-9\n",
        "\n",
        "# ---- run HYBRID CAM for each image ----\n",
        "for i, img_path in enumerate(image_paths):\n",
        "    bgr = cv2.imread(img_path)\n",
        "    if bgr is None:\n",
        "        print(\"Skip (cannot read):\", img_path)\n",
        "        continue\n",
        "\n",
        "    bgr640 = letterbox(bgr, new_shape=640)\n",
        "    x, rgb_float = to_tensor_yolo(bgr640, device=device)\n",
        "\n",
        "    x.requires_grad_(True)\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    cam_map8  = cam8(input_tensor=x,  targets=[YoloMaxScoreTarget()])[0]   # [640,640]\n",
        "    cam_map11 = cam11(input_tensor=x, targets=[YoloMaxScoreTarget()])[0]   # [640,640]\n",
        "\n",
        "    hybrid_cam = (w8_cam * cam_map8 + w11_cam * cam_map11) / w_sum\n",
        "\n",
        "    overlay = show_cam_on_image(rgb_float, hybrid_cam, use_rgb=True)\n",
        "    overlay_bgr = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    out_name = os.path.splitext(os.path.basename(img_path))[0] + \"_HYBRID_gradcampp.jpg\"\n",
        "    out_path = os.path.join(cam_dir, out_name)\n",
        "    cv2.imwrite(out_path, overlay_bgr)\n",
        "\n",
        "    print(f\"[{i+1}/{len(image_paths)}] Saved:\", out_path)\n",
        "\n",
        "print(\"✅ Hybrid Grad-CAM++ outputs in:\", cam_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izKB6M9BbuIW",
        "outputId": "f0c05498-bbc2-4b05-f066-77e58619f70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8 CAM layer : Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "YOLOv11 CAM layer: Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "[1/2] Saved: /content/hybrid_out/hybrid_gradcampp/emotion_female_HYBRID_gradcampp.jpg\n",
            "[2/2] Saved: /content/hybrid_out/hybrid_gradcampp/emotion_male_HYBRID_gradcampp.jpg\n",
            "✅ Hybrid Grad-CAM++ outputs in: /content/hybrid_out/hybrid_gradcampp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "cams = sorted(glob.glob(\"/content/hybrid_out/hybrid_gradcampp*.jpg\"))\n",
        "preds = sorted(glob.glob(\"/content/hybrid_out/pred*.jpg\"))\n",
        "\n",
        "print(\"CAMs:\", len(cams))\n",
        "for p in cams:\n",
        "    img = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(os.path.basename(p))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qJRb_VYYbzfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361545df-507a-41b4-9c18-c7df97ba48e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAMs: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Explainability Utilities for YOLO / Hybrid YOLO (clean rewrite)\n",
        "# Installs, imports, helper funcs, and faithfulness metrics:\n",
        "# - Occlusion Sensitivity Map\n",
        "# - Deletion / Insertion Curves\n",
        "# ============================================================\n",
        "import os, glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG\n",
        "# -------------------------\n",
        "OUT_BASE = \"/content/outputs/explainability_plots\"\n",
        "os.makedirs(OUT_BASE, exist_ok=True)\n",
        "\n",
        "# If you already defined `model` and `image_paths`, keep them.\n",
        "# Example:\n",
        "# model = YOLO(\"/content/best.pt\")\n",
        "# image_paths = sorted(glob.glob(\"/content/test_imgs/*.jpg\"))\n",
        "\n",
        "# -------------------------\n",
        "# Filesystem helpers\n",
        "# -------------------------\n",
        "def ensure_dir(path: str) -> str:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def save_bgr(path: str, bgr: np.ndarray) -> None:\n",
        "    cv2.imwrite(path, bgr)\n",
        "\n",
        "def save_fig(path: str) -> None:\n",
        "    plt.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "# -------------------------\n",
        "# Preprocess helpers\n",
        "# -------------------------\n",
        "def letterbox(im: np.ndarray, new_shape: int = 640, color=(114,114,114)) -> np.ndarray:\n",
        "    \"\"\"Square letterbox resize (YOLO-style) to new_shape x new_shape.\"\"\"\n",
        "    h, w = im.shape[:2]\n",
        "    r = min(new_shape / h, new_shape / w)\n",
        "    nh, nw = int(round(h * r)), int(round(w * r))\n",
        "    im_resized = cv2.resize(im, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    canvas = np.full((new_shape, new_shape, 3), color, dtype=np.uint8)\n",
        "    top = (new_shape - nh) // 2\n",
        "    left = (new_shape - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = im_resized\n",
        "    return canvas\n",
        "\n",
        "def bgr_to_rgb01(bgr: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"BGR uint8 -> RGB float32 in [0,1].\"\"\"\n",
        "    return cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "# -------------------------\n",
        "# Visualization helpers\n",
        "# -------------------------\n",
        "def draw_yolo_boxes(bgr: np.ndarray, res, color=(0,255,0), thickness: int = 2) -> np.ndarray:\n",
        "    \"\"\"Draw Ultralytics YOLO boxes on a BGR image.\"\"\"\n",
        "    out = bgr.copy()\n",
        "    if res is None or res.boxes is None or len(res.boxes) == 0:\n",
        "        return out\n",
        "\n",
        "    xyxy = res.boxes.xyxy.detach().cpu().numpy().astype(int)\n",
        "    conf = res.boxes.conf.detach().cpu().numpy()\n",
        "    cls  = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "    names = getattr(res, \"names\", None)\n",
        "\n",
        "    for (x1,y1,x2,y2), c, k in zip(xyxy, conf, cls):\n",
        "        cv2.rectangle(out, (x1,y1), (x2,y2), color, thickness)\n",
        "        cname = names[k] if (isinstance(names, dict) and k in names) else (names[k] if isinstance(names, list) else str(k))\n",
        "        cv2.putText(out, f\"{cname}: {c:.2f}\", (x1, max(15, y1-5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    return out\n",
        "\n",
        "def normalize01(x: np.ndarray) -> np.ndarray:\n",
        "    x = x.astype(np.float32)\n",
        "    x = x - x.min()\n",
        "    return x / (x.max() - x.min() + 1e-9)\n",
        "\n",
        "def cam_to_heatmap_bgr(cam_2d: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"2D CAM -> BGR heatmap (JET).\"\"\"\n",
        "    cam01 = (normalize01(cam_2d) * 255).astype(np.uint8)\n",
        "    return cv2.applyColorMap(cam01, cv2.COLORMAP_JET)\n",
        "\n",
        "def contour_overlay(bgr: np.ndarray, cam_2d: np.ndarray, levels=(0.4, 0.6, 0.8)) -> np.ndarray:\n",
        "    \"\"\"Overlay contours at CAM thresholds.\"\"\"\n",
        "    cam01 = normalize01(cam_2d)\n",
        "    out = bgr.copy()\n",
        "    for lv in levels:\n",
        "        mask = (cam01 >= lv).astype(np.uint8) * 255\n",
        "        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cv2.drawContours(out, cnts, -1, (0,0,255), 2)\n",
        "    return out\n",
        "\n",
        "def threshold_mask(cam_2d: np.ndarray, thr: float = 0.6) -> np.ndarray:\n",
        "    \"\"\"Binary mask from CAM.\"\"\"\n",
        "    return (normalize01(cam_2d) >= thr).astype(np.uint8)\n",
        "\n",
        "def montage3(rgb_list, titles, out_path: str) -> None:\n",
        "    \"\"\"Save a 1x3 montage (RGB images expected).\"\"\"\n",
        "    plt.figure(figsize=(15,5))\n",
        "    for i, (im, t) in enumerate(zip(rgb_list, titles), 1):\n",
        "        plt.subplot(1,3,i)\n",
        "        plt.imshow(im)\n",
        "        plt.title(t)\n",
        "        plt.axis(\"off\")\n",
        "    save_fig(out_path)\n",
        "\n",
        "# ============================================================\n",
        "# Faithfulness scoring for YOLO / Hybrid YOLO\n",
        "# (Uses model.predict; suitable for occlusion + del/ins curves.)\n",
        "# ============================================================\n",
        "\n",
        "def yolo_top_score(model, img_bgr: np.ndarray, imgsz: int = 640, conf: float = 0.25, device=0) -> float:\n",
        "    \"\"\"Return top confidence among all predicted boxes for an image.\"\"\"\n",
        "    r = model.predict(source=img_bgr, imgsz=imgsz, conf=conf, device=device, verbose=False)[0]\n",
        "    if r.boxes is None or len(r.boxes) == 0:\n",
        "        return 0.0\n",
        "    return float(r.boxes.conf.max().detach().cpu().item())\n",
        "\n",
        "def occlusion_sensitivity_map(model,\n",
        "                             img_bgr: np.ndarray,\n",
        "                             patch: int = 64,\n",
        "                             stride: int = 64,\n",
        "                             imgsz: int = 640,\n",
        "                             conf: float = 0.25,\n",
        "                             device=0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Occlusion sensitivity:\n",
        "    - compute base score\n",
        "    - black-out patches and measure score drop\n",
        "    - upsample to image size\n",
        "    Returns: HxW float map in [0,1]\n",
        "    \"\"\"\n",
        "    base = yolo_top_score(model, img_bgr, imgsz=imgsz, conf=conf, device=device)\n",
        "    H, W = img_bgr.shape[:2]\n",
        "\n",
        "    oh = int(np.ceil((H - patch) / stride)) + 1\n",
        "    ow = int(np.ceil((W - patch) / stride)) + 1\n",
        "    m = np.zeros((oh, ow), dtype=np.float32)\n",
        "\n",
        "    for yi in range(oh):\n",
        "        for xi in range(ow):\n",
        "            y1, x1 = yi * stride, xi * stride\n",
        "            y2, x2 = min(H, y1 + patch), min(W, x1 + patch)\n",
        "\n",
        "            masked = img_bgr.copy()\n",
        "            masked[y1:y2, x1:x2] = 0  # black mask\n",
        "\n",
        "            s = yolo_top_score(model, masked, imgsz=imgsz, conf=conf, device=device)\n",
        "            m[yi, xi] = max(0.0, base - s)\n",
        "\n",
        "    m = normalize01(m)\n",
        "    return cv2.resize(m, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "def deletion_insertion_curves(model,\n",
        "                              img_bgr: np.ndarray,\n",
        "                              importance_map: np.ndarray,\n",
        "                              steps: int = 20,\n",
        "                              imgsz: int = 640,\n",
        "                              conf: float = 0.25,\n",
        "                              device=0):\n",
        "    \"\"\"\n",
        "    Deletion: progressively remove most important pixels (set to 0).\n",
        "    Insertion: start blurred, progressively insert important pixels.\n",
        "    Returns: fracs, del_scores, ins_scores\n",
        "    \"\"\"\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    imp = importance_map.reshape(-1)\n",
        "    order = np.argsort(-imp)  # most important first\n",
        "\n",
        "    rgb = bgr_to_rgb01(img_bgr)\n",
        "    flat = rgb.reshape(-1, 3)\n",
        "\n",
        "    blur = cv2.GaussianBlur(img_bgr, (31,31), 0)\n",
        "    blur_rgb = bgr_to_rgb01(blur)\n",
        "    flat_blur = blur_rgb.reshape(-1, 3)\n",
        "\n",
        "    del_scores, ins_scores, fracs = [], [], []\n",
        "    n = flat.shape[0]\n",
        "\n",
        "    for t in range(steps + 1):\n",
        "        frac = t / steps\n",
        "        k = int(frac * n)\n",
        "\n",
        "        # Deletion\n",
        "        del_flat = flat.copy()\n",
        "        del_flat[order[:k]] = 0.0\n",
        "        del_img = (del_flat.reshape(H, W, 3) * 255).astype(np.uint8)\n",
        "        del_bgr = cv2.cvtColor(del_img, cv2.COLOR_RGB2BGR)\n",
        "        del_scores.append(yolo_top_score(model, del_bgr, imgsz=imgsz, conf=conf, device=device))\n",
        "\n",
        "        # Insertion\n",
        "        ins_flat = flat_blur.copy()\n",
        "        ins_flat[order[:k]] = flat[order[:k]]\n",
        "        ins_img = (ins_flat.reshape(H, W, 3) * 255).astype(np.uint8)\n",
        "        ins_bgr = cv2.cvtColor(ins_img, cv2.COLOR_RGB2BGR)\n",
        "        ins_scores.append(yolo_top_score(model, ins_bgr, imgsz=imgsz, conf=conf, device=device))\n",
        "\n",
        "        fracs.append(frac)\n",
        "\n",
        "    return np.array(fracs), np.array(del_scores), np.array(ins_scores)"
      ],
      "metadata": {
        "id": "sOyhiS_Pjv1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Hybrid Explainability Post-processing & Visualization (ONE BLOCK)\n",
        "# Uses:\n",
        "#  - Hybrid Grad-CAM++ overlays: /content/hybrid_out/hybrid_gradcampp\n",
        "#  - Hybrid/YOLO annotated preds: /content/hybrid_out/pred (optional)\n",
        "# Produces per-image explainability assets:\n",
        "#  - Original, Pred annotated, Hybrid CAM overlay + derived plots\n",
        "#  - Occlusion sensitivity heatmap + overlay\n",
        "#  - Deletion/Insertion faithfulness curves\n",
        "# Saves into:\n",
        "#  - /content/hybrid_out/explainability_plots/<image_name>/\n",
        "#\n",
        "# REQUIREMENTS (must already exist in notebook):\n",
        "#   - image_paths: list of image file paths\n",
        "#   - m8, m11: ultralytics.YOLO objects (best_y8.pt, best_y11.pt)\n",
        "#   - If you want hybrid scoring/prediction: ensemble-boxes installed\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "\n",
        "# -----------------------------\n",
        "# PATHS (as you requested)\n",
        "# -----------------------------\n",
        "CAM_DIR_EXISTING = \"/content/hybrid_out/hybrid_gradcampp\"\n",
        "PRED_DIR         = \"/content/hybrid_out/pred\"\n",
        "OUT_BASE         = \"/content/hybrid_out/explainability_plots\"\n",
        "os.makedirs(OUT_BASE, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Small utils (self-contained)\n",
        "# -----------------------------\n",
        "def ensure_dir(path: str) -> str:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def save_img(path: str, bgr: np.ndarray) -> None:\n",
        "    cv2.imwrite(path, bgr)\n",
        "\n",
        "def save_matplotlib_figure(path: str) -> None:\n",
        "    plt.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "def normalize_0_1(x: np.ndarray) -> np.ndarray:\n",
        "    x = x.astype(np.float32)\n",
        "    x = x - x.min()\n",
        "    return x / (x.max() - x.min() + 1e-9)\n",
        "\n",
        "def heatmap_bgr_from_cam(cam_2d: np.ndarray) -> np.ndarray:\n",
        "    cam01 = (normalize_0_1(cam_2d) * 255).astype(np.uint8)\n",
        "    return cv2.applyColorMap(cam01, cv2.COLORMAP_JET)  # BGR\n",
        "\n",
        "def threshold_mask(cam_2d: np.ndarray, thr: float = 0.6) -> np.ndarray:\n",
        "    cam01 = normalize_0_1(cam_2d)\n",
        "    return (cam01 >= thr).astype(np.uint8)\n",
        "\n",
        "def contour_overlay(bgr: np.ndarray, cam_2d: np.ndarray, levels=(0.4, 0.6, 0.8)) -> np.ndarray:\n",
        "    cam01 = normalize_0_1(cam_2d)\n",
        "    out = bgr.copy()\n",
        "    for lv in levels:\n",
        "        mask = (cam01 >= lv).astype(np.uint8) * 255\n",
        "        cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        cv2.drawContours(out, cnts, -1, (0, 0, 255), 2)\n",
        "    return out\n",
        "\n",
        "def montage3(rgb_list, titles, out_path: str) -> None:\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, (im, t) in enumerate(zip(rgb_list, titles), 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        plt.imshow(im)\n",
        "        plt.title(t)\n",
        "        plt.axis(\"off\")\n",
        "    save_matplotlib_figure(out_path)\n",
        "\n",
        "def draw_yolo_boxes(bgr: np.ndarray, xyxy: np.ndarray, conf: np.ndarray, cls: np.ndarray, names, color=(0,255,0), thickness=2):\n",
        "    out = bgr.copy()\n",
        "    for (x1,y1,x2,y2), c, k in zip(xyxy, conf, cls):\n",
        "        x1,y1,x2,y2 = map(int, [x1,y1,x2,y2])\n",
        "        cv2.rectangle(out, (x1,y1), (x2,y2), color, thickness)\n",
        "        cname = names[k] if isinstance(names, dict) else names[k]\n",
        "        cv2.putText(out, f\"{cname}: {c:.2f}\", (x1, max(15, y1-5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# HYBRID model wrapper (WBF)\n",
        "# -----------------------------\n",
        "class HybridYOLO:\n",
        "    \"\"\"\n",
        "    Hybrid YOLO (YOLOv8 + YOLOv11) with Weighted Boxes Fusion.\n",
        "    Provides:\n",
        "      - predict(): returns a lightweight 'result' object similar enough for our needs\n",
        "      - top_score(): scalar for faithfulness metrics\n",
        "    \"\"\"\n",
        "    def __init__(self, m8, m11, wbf_weights=(1.0, 1.0), iou_thr=0.55, skip_thr=0.001):\n",
        "        self.m8 = m8\n",
        "        self.m11 = m11\n",
        "        self.wbf_weights = tuple(wbf_weights)\n",
        "        self.iou_thr = float(iou_thr)\n",
        "        self.skip_thr = float(skip_thr)\n",
        "        self.names = getattr(m8, \"names\", None)\n",
        "        self.device = 0  # Colab GPU id\n",
        "\n",
        "    def _res_to_norm(self, res, w, h):\n",
        "        if res.boxes is None or len(res.boxes) == 0:\n",
        "            return [], [], []\n",
        "        boxes = res.boxes.xyxy.detach().cpu().numpy().astype(np.float32)\n",
        "        scores = res.boxes.conf.detach().cpu().numpy().astype(np.float32)\n",
        "        labels = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "\n",
        "        boxes[:, [0,2]] /= float(w)\n",
        "        boxes[:, [1,3]] /= float(h)\n",
        "        boxes = np.clip(boxes, 0.0, 1.0)\n",
        "        return boxes.tolist(), scores.tolist(), labels.tolist()\n",
        "\n",
        "    def _fuse(self, res8, res11, w, h):\n",
        "        b8, s8, l8   = self._res_to_norm(res8,  w, h)\n",
        "        b11, s11, l11 = self._res_to_norm(res11, w, h)\n",
        "\n",
        "        boxes, scores, labels = weighted_boxes_fusion(\n",
        "            [b8, b11],\n",
        "            [s8, s11],\n",
        "            [l8, l11],\n",
        "            weights=list(self.wbf_weights),\n",
        "            iou_thr=self.iou_thr,\n",
        "            skip_box_thr=self.skip_thr\n",
        "        )\n",
        "\n",
        "        boxes = np.asarray(boxes, dtype=np.float32)\n",
        "        scores = np.asarray(scores, dtype=np.float32)\n",
        "        labels = np.asarray(labels, dtype=int)\n",
        "\n",
        "        if boxes.size:\n",
        "            boxes[:, [0,2]] *= float(w)\n",
        "            boxes[:, [1,3]] *= float(h)\n",
        "        return boxes, scores, labels\n",
        "\n",
        "    def predict(self, source, imgsz=640, conf=0.25, device=0, verbose=False):\n",
        "        # source can be path or BGR ndarray\n",
        "        if isinstance(source, str):\n",
        "            bgr = cv2.imread(source)\n",
        "            if bgr is None:\n",
        "                return [self._empty_result()]\n",
        "        else:\n",
        "            bgr = source\n",
        "\n",
        "        h, w = bgr.shape[:2]\n",
        "\n",
        "        r8  = self.m8.predict(source=bgr, imgsz=imgsz, conf=0.001, device=device, verbose=verbose)[0]\n",
        "        r11 = self.m11.predict(source=bgr, imgsz=imgsz, conf=0.001, device=device, verbose=verbose)[0]\n",
        "\n",
        "        boxes, scores, labels = self._fuse(r8, r11, w, h)\n",
        "\n",
        "        # apply final conf threshold\n",
        "        keep = scores >= float(conf)\n",
        "        boxes = boxes[keep]\n",
        "        scores = scores[keep]\n",
        "        labels = labels[keep]\n",
        "\n",
        "        return [self._wrap_result(boxes, scores, labels, names=self.names)]\n",
        "\n",
        "    def top_score(self, source, imgsz=640, conf=0.25, device=0):\n",
        "        res = self.predict(source=source, imgsz=imgsz, conf=conf, device=device, verbose=False)[0]\n",
        "        if res.boxes is None or len(res.boxes) == 0:\n",
        "            return 0.0\n",
        "        return float(res.boxes.conf.max())\n",
        "\n",
        "    # --- lightweight result structure compatible with our drawing ---\n",
        "    class _Boxes:\n",
        "        def __init__(self, xyxy, conf, cls):\n",
        "            self.xyxy = xyxy\n",
        "            self.conf = conf\n",
        "            self.cls  = cls\n",
        "        def __len__(self):\n",
        "            return 0 if self.xyxy is None else len(self.xyxy)\n",
        "\n",
        "    class _Result:\n",
        "        def __init__(self, boxes, names):\n",
        "            self.boxes = boxes\n",
        "            self.names = names\n",
        "\n",
        "    def _wrap_result(self, xyxy, conf, cls, names):\n",
        "        return HybridYOLO._Result(\n",
        "            HybridYOLO._Boxes(xyxy, conf, cls),\n",
        "            names\n",
        "        )\n",
        "\n",
        "    def _empty_result(self):\n",
        "        return HybridYOLO._Result(HybridYOLO._Boxes(None, None, None), self.names)\n",
        "\n",
        "# -----------------------------\n",
        "# Faithfulness metrics (hybrid)\n",
        "# -----------------------------\n",
        "def yolo_top_score(model_like, img_bgr, imgsz=640, conf=0.25, device=0):\n",
        "    # works for HybridYOLO (has top_score) and Ultralytics YOLO (predict)\n",
        "    if hasattr(model_like, \"top_score\"):\n",
        "        return model_like.top_score(img_bgr, imgsz=imgsz, conf=conf, device=device)\n",
        "    r = model_like.predict(source=img_bgr, imgsz=imgsz, conf=conf, device=device, verbose=False)[0]\n",
        "    if r.boxes is None or len(r.boxes) == 0:\n",
        "        return 0.0\n",
        "    return float(np.max(r.boxes.conf))\n",
        "\n",
        "def occlusion_sensitivity_map(model_like, img_bgr, patch=64, stride=64, imgsz=640, conf=0.25, device=0):\n",
        "    base = yolo_top_score(model_like, img_bgr, imgsz=imgsz, conf=conf, device=device)\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    oh = int(np.ceil((H - patch) / stride)) + 1\n",
        "    ow = int(np.ceil((W - patch) / stride)) + 1\n",
        "    m = np.zeros((oh, ow), dtype=np.float32)\n",
        "\n",
        "    for yi in range(oh):\n",
        "        for xi in range(ow):\n",
        "            y1, x1 = yi * stride, xi * stride\n",
        "            y2, x2 = min(H, y1 + patch), min(W, x1 + patch)\n",
        "            masked = img_bgr.copy()\n",
        "            masked[y1:y2, x1:x2] = 0\n",
        "            s = yolo_top_score(model_like, masked, imgsz=imgsz, conf=conf, device=device)\n",
        "            m[yi, xi] = max(0.0, base - s)\n",
        "\n",
        "    m = normalize_0_1(m)\n",
        "    return cv2.resize(m, (W, H), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "def deletion_insertion_curves(model_like, img_bgr, importance_map, steps=20, imgsz=640, conf=0.25, device=0):\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    imp = importance_map.reshape(-1)\n",
        "    order = np.argsort(-imp)\n",
        "\n",
        "    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    flat = rgb.reshape(-1, 3)\n",
        "\n",
        "    blur = cv2.GaussianBlur(img_bgr, (31, 31), 0)\n",
        "    blur_rgb = cv2.cvtColor(blur, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    flat_blur = blur_rgb.reshape(-1, 3)\n",
        "\n",
        "    del_scores, ins_scores, fracs = [], [], []\n",
        "    n = flat.shape[0]\n",
        "\n",
        "    for t in range(steps + 1):\n",
        "        frac = t / steps\n",
        "        k = int(frac * n)\n",
        "\n",
        "        # deletion\n",
        "        del_flat = flat.copy()\n",
        "        del_flat[order[:k]] = 0.0\n",
        "        del_img = (del_flat.reshape(H, W, 3) * 255).astype(np.uint8)\n",
        "        del_bgr = cv2.cvtColor(del_img, cv2.COLOR_RGB2BGR)\n",
        "        del_scores.append(yolo_top_score(model_like, del_bgr, imgsz=imgsz, conf=conf, device=device))\n",
        "\n",
        "        # insertion\n",
        "        ins_flat = flat_blur.copy()\n",
        "        ins_flat[order[:k]] = flat[order[:k]]\n",
        "        ins_img = (ins_flat.reshape(H, W, 3) * 255).astype(np.uint8)\n",
        "        ins_bgr = cv2.cvtColor(ins_img, cv2.COLOR_RGB2BGR)\n",
        "        ins_scores.append(yolo_top_score(model_like, ins_bgr, imgsz=imgsz, conf=conf, device=device))\n",
        "\n",
        "        fracs.append(frac)\n",
        "\n",
        "    return np.array(fracs), np.array(del_scores), np.array(ins_scores)\n",
        "\n",
        "# -----------------------------\n",
        "# Define HYBRID model for this script\n",
        "# (If you already have wbf_weights from earlier, it will use it;\n",
        "#  otherwise defaults to (1.0, 1.0))\n",
        "# -----------------------------\n",
        "try:\n",
        "    _wbf_weights = wbf_weights  # if you defined earlier\n",
        "except Exception:\n",
        "    _wbf_weights = (1.0, 1.0)\n",
        "\n",
        "model = HybridYOLO(m8, m11, wbf_weights=_wbf_weights, iou_thr=0.55, skip_thr=0.001)\n",
        "\n",
        "# -----------------------------\n",
        "# Main loop\n",
        "# -----------------------------\n",
        "for img_path in image_paths:\n",
        "    name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    outdir = ensure_dir(os.path.join(OUT_BASE, name))\n",
        "\n",
        "    orig_bgr = cv2.imread(img_path)\n",
        "    if orig_bgr is None:\n",
        "        print(\"Skip (cannot read):\", img_path)\n",
        "        continue\n",
        "\n",
        "    # 1) Hybrid prediction (WBF fused)\n",
        "    res = model.predict(source=img_path, imgsz=640, conf=0.25, device=0, verbose=False)[0]\n",
        "    if res.boxes is None or len(res.boxes) == 0:\n",
        "        pred_drawn = orig_bgr.copy()\n",
        "    else:\n",
        "        pred_drawn = draw_yolo_boxes(\n",
        "            orig_bgr,\n",
        "            res.boxes.xyxy,\n",
        "            res.boxes.conf,\n",
        "            res.boxes.cls,\n",
        "            names=res.names\n",
        "        )\n",
        "\n",
        "    # 2) Load existing Hybrid Grad-CAM++ overlay (if exists)\n",
        "    cam_overlay_path = os.path.join(CAM_DIR_EXISTING, f\"{name}_HYBRID_gradcampp.jpg\")\n",
        "    cam_overlay_bgr = cv2.imread(cam_overlay_path) if os.path.exists(cam_overlay_path) else None\n",
        "\n",
        "    # Save base\n",
        "    save_img(os.path.join(outdir, \"01_original.jpg\"), orig_bgr)\n",
        "    save_img(os.path.join(outdir, \"02_pred_annotated.jpg\"), pred_drawn)\n",
        "\n",
        "    # 3) CAM-based visualizations (if CAM exists)\n",
        "    if cam_overlay_bgr is not None:\n",
        "        save_img(os.path.join(outdir, \"03_hybrid_gradcampp_overlay.jpg\"), cam_overlay_bgr)\n",
        "\n",
        "        o = cv2.resize(orig_bgr, (cam_overlay_bgr.shape[1], cam_overlay_bgr.shape[0]))\n",
        "        diff = cv2.absdiff(cam_overlay_bgr, o)\n",
        "        diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "        cam_approx = normalize_0_1(diff_gray)\n",
        "\n",
        "        hm_bgr = heatmap_bgr_from_cam(cam_approx)\n",
        "        save_img(os.path.join(outdir, \"04_cam_heatmap_only.jpg\"), hm_bgr)\n",
        "\n",
        "        plt.figure(figsize=(6,5))\n",
        "        plt.imshow(cam_approx, cmap=\"jet\")\n",
        "        plt.colorbar()\n",
        "        plt.title(\"Hybrid CAM intensity (approx)\")\n",
        "        plt.axis(\"off\")\n",
        "        save_matplotlib_figure(os.path.join(outdir, \"05_cam_with_colorbar.png\"))\n",
        "\n",
        "        mask = (threshold_mask(cam_approx, thr=0.6) * 255).astype(np.uint8)\n",
        "        save_img(os.path.join(outdir, \"06_cam_threshold_mask.jpg\"), mask)\n",
        "\n",
        "        cont = contour_overlay(o, cam_approx, levels=(0.4, 0.6, 0.8))\n",
        "        save_img(os.path.join(outdir, \"07_cam_contours.jpg\"), cont)\n",
        "\n",
        "        # draw boxes on CAM overlay\n",
        "        if res.boxes is not None and len(res.boxes) > 0:\n",
        "            cam_bbox = draw_yolo_boxes(\n",
        "                cam_overlay_bgr,\n",
        "                res.boxes.xyxy,\n",
        "                res.boxes.conf,\n",
        "                res.boxes.cls,\n",
        "                names=res.names,\n",
        "                color=(0,255,255)\n",
        "            )\n",
        "        else:\n",
        "            cam_bbox = cam_overlay_bgr.copy()\n",
        "        save_img(os.path.join(outdir, \"08_cam_overlay_with_bboxes.jpg\"), cam_bbox)\n",
        "\n",
        "        rgb_orig = cv2.cvtColor(orig_bgr, cv2.COLOR_BGR2RGB)\n",
        "        rgb_pred = cv2.cvtColor(pred_drawn, cv2.COLOR_BGR2RGB)\n",
        "        rgb_cam  = cv2.cvtColor(cam_overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "        montage3([rgb_orig, rgb_pred, rgb_cam],\n",
        "                 [\"Original\", \"Hybrid Prediction\", \"Hybrid Grad-CAM++\"],\n",
        "                 os.path.join(outdir, \"09_montage.png\"))\n",
        "\n",
        "        cy, cx = np.unravel_index(np.argmax(cam_approx), cam_approx.shape)\n",
        "        H, W = o.shape[:2]\n",
        "        crop_sz = min(H, W) // 2\n",
        "        y1, y2 = max(0, cy - crop_sz//2), min(H, cy + crop_sz//2)\n",
        "        x1, x2 = max(0, cx - crop_sz//2), min(W, cx + crop_sz//2)\n",
        "        crop = o[y1:y2, x1:x2]\n",
        "        save_img(os.path.join(outdir, \"10_top_activation_crop.jpg\"), crop)\n",
        "\n",
        "    # 4) Occlusion sensitivity (faithfulness) using HYBRID score\n",
        "    occ = occlusion_sensitivity_map(model, orig_bgr, patch=64, stride=64, imgsz=640, conf=0.25, device=0)\n",
        "    occ_hm = heatmap_bgr_from_cam(occ)\n",
        "    occ_overlay = cv2.addWeighted(orig_bgr, 0.55, occ_hm, 0.45, 0)\n",
        "    save_img(os.path.join(outdir, \"11_occlusion_heatmap.jpg\"), occ_hm)\n",
        "    save_img(os.path.join(outdir, \"12_occlusion_overlay.jpg\"), occ_overlay)\n",
        "\n",
        "    # 5) Deletion & insertion curves using occlusion as importance\n",
        "    fracs, del_s, ins_s = deletion_insertion_curves(model, orig_bgr, occ, steps=20, imgsz=640, conf=0.25, device=0)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(fracs, del_s, label=\"Deletion (remove important)\")\n",
        "    plt.plot(fracs, ins_s, label=\"Insertion (add important)\")\n",
        "    plt.xlabel(\"Fraction of pixels\")\n",
        "    plt.ylabel(\"Top detection score\")\n",
        "    plt.title(\"Faithfulness Curves (Hybrid YOLO)\")\n",
        "    plt.legend()\n",
        "    save_matplotlib_figure(os.path.join(outdir, \"13_deletion_insertion_curves.png\"))\n",
        "\n",
        "    print(\"✅ Saved all explainability outputs to:\", outdir)\n",
        "\n",
        "print(\"DONE. Master folder:\", OUT_BASE)"
      ],
      "metadata": {
        "id": "YziQxSExkBgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad3bdb3-0b85-43cf-af2c-6d93dc4d06d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved all explainability outputs to: /content/hybrid_out/explainability_plots/emotion_female\n",
            "✅ Saved all explainability outputs to: /content/hybrid_out/explainability_plots/emotion_male\n",
            "DONE. Master folder: /content/hybrid_out/explainability_plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Replace 'your_folder' with the folder you want to download\n",
        "folder_name = \"/content/hybrid_out\"\n",
        "zip_name = f\"{folder_name}.zip\"\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(folder_name, 'zip', folder_name)\n",
        "\n",
        "# Download the zipped folder\n",
        "files.download(zip_name)"
      ],
      "metadata": {
        "id": "G_P-Z7kBkCOw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ae1a6f56-efc1-481d-f965-4fbf44b18ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8acb303c-3d98-4ce2-a98f-11189ffb7590\", \"hybrid_out.zip\", 5553412)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Replace 'your_folder' with the folder you want to download\n",
        "folder_name = \"/content/outputs\"\n",
        "zip_name = f\"{folder_name}.zip\"\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(folder_name, 'zip', folder_name)\n",
        "\n",
        "# Download the zipped folder\n",
        "files.download(zip_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "t6zh5562vwp7",
        "outputId": "7c97459f-1a15-4ae7-8585-ee38b936b83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_895af6f4-9454-46d0-a0ce-fc3c8b2d3aa6\", \"outputs.zip\", 62268521)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}